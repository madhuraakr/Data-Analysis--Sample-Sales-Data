# -*- coding: utf-8 -*-
"""Clustering(kmeans,kmodes).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EdmItYZ7Qx27lhFVYhuE10q3wImRvEvO

Program 11 -Clustering (Distance based methods)
##### 23/04/2021

**Kmeans**
###### Dataset- Sample Sales Data
---



Customer Segmentation is the process of dividing customer into differnt groups based on their characteristics for effictive marketing purposes.

RFM (recency, frequency, monetary) analysis is a marketing technique used to determine quantitatively which customers are the best ones by examining how recently a customer has purchased (recency), how often they purchase (frequency), and how much the customer spends (monetary).
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
#datetime
import datetime as dt
from statsmodels.graphics.gofplots import qqplot
#StandardSccaler
from sklearn.preprocessing import StandardScaler
#KMeans
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D
from yellowbrick.cluster import KElbowVisualizer
sns.set(rc={'figure.figsize':(5,4)})

"""##### Collecting data

The provided data contains purchase information of a fictional store named Steel Wheels (available at https://www.kaggle.com/kyanyoga/sample-sales-data). Each line refers to a specific product that was sold to a customer in a specific order. 

* ORDERNUMBER: Unique ID for each order made by a customer
* ORDERDATE: Date of the order
* PRODUCTLINE: Type of product
* QUANTITYORDERED: Quantity of the product item included in the considered order specified by ORDERNUMBER
* SALES: Total price of the product item included in the considered order
* CUSTOMERNAME: Name of the customer of the considered order
"""

from google.colab import drive
drive.mount('/content/drive')

df= pd.read_csv('/content/drive/My Drive/sales_data_sample.csv',encoding= 'unicode_escape')
df.head()

#Shape of the dataset
df.shape

df.columns

"""##### Data Cleaning and Data Exploratory Analysis """

to_drop = ['PHONE','ADDRESSLINE1','ADDRESSLINE2','STATE','POSTALCODE','TERRITORY']
df = df.drop(to_drop, axis=1)

"""Here, we remove the variables which dont add significant value for the analysis- 'PHONE','ADDRESSLINE1','ADDRESSLINE2','STATE','POSTALCODE','TERRITORY'"""

#Finding missing values
df.isnull().sum()

"""The above dataframe shows that the data does not have any missing values and need no data imputation.

#### Outlier detection and removal
"""

df.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))
plt.show()

"""Itcan be seen that 'MSRP' has a single outlier, while the 'SALES' and 'QUANTITYORDERED' has the most number of outliers.Most of the outliers can be filtered using the filtering criteria of a boxplot to classify the point as an outlier. The criteria of a box plot for classifying a point as an outlier is if the point is greater than  Q3+(1.5∗IQR)  or lower than  Q1−(1.5∗IQR)  where, where  Q1=FirstQuartile   Q3=ThirdQuartile. """

print('original shape of dataset :',df.shape)
cols = ['SALES', 'MSRP','QUANTITYORDERED']
new_df = df[cols]

#criteria
Q1 = new_df.quantile(0.25)
Q3 = new_df.quantile(0.75)
IQR = Q3-Q1
max_ = Q3+1.5*IQR
min_ = Q1-1.5*IQR
#filter the outlier s
condition = (new_df <= max_) & (new_df >= min_)
condition = condition.all(axis=1)
df = df[condition]
print('filtered dataset shape : ',df.shape)
df.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))
plt.show()

"""After the outlier filtering, only a few outliers are existing. Therefore, we move on with the analysis."""

#Checking for inconsistent data types
df.dtypes

#Changing the data type of variable 'ORDERDATE' from object to datetime
df['ORDERDATE'] = pd.to_datetime(df['ORDERDATE'])

"""#### Summary stats of Quantitative variables"""

quant_vars = ['QUANTITYORDERED','PRICEEACH','SALES','MSRP']
df[quant_vars].describe()

"""It can be seen that the quantitative variables consists of no negative values, which is a good indicator as the above mentioned variables cannot have negative values.

#### Order Quantity Distribution
"""

plt.figure(figsize=(5,4))
sns.distplot(df['QUANTITYORDERED'])
plt.title('Order Quantity Distribution')
plt.xlabel('Quantity Ordered')
plt.ylabel('Frequency')
plt.show()

"""Most of the orders' quantity lie between 20 and 40 units. Therefore, majority of them are bulk orders.

#### Price Distribution
"""

plt.figure(figsize=(5,4))
sns.distplot(df['PRICEEACH'])
plt.title('Price Distribution')
plt.xlabel('Price Ordered')
plt.ylabel('Frequency')
plt.show()

"""The distribution is left skewed. Meaning, most of the orders placed are of quantities costing between 90$ and 100$ per item. Also, the maximum price per item in the records is 100$.

#### Sales Distribution
"""

plt.figure(figsize=(5,4))
sns.distplot(df['SALES'])
plt.title('Sales Distribution')
plt.xlabel('Sales')
plt.ylabel('Frequency')
plt.show()

"""The distribution is right skewed.Majority of the customer sales are below the mean sales of the time period considered.

#### Analyzing 'STATUS' of the orders
"""

df['STATUS'].value_counts()

df['STATUS'].value_counts(normalize = True)

plt.figure(figsize=(5,4))
sns.countplot(x="STATUS",data=df,linewidth=2,edgecolor=sns.color_palette("dark", 1))

plt.figure(figsize=(5,4))
sns.countplot(y='STATUS',data=df,hue='YEAR_ID')

"""A huge majority of the orders/products have been successfully shipped. While, there was Dispute, In process and Onhold status in 2005. This highlights the performance of the company shipping in 2005."""

df.groupby(['YEAR_ID'])['MONTH_ID'].nunique()

"""The above output highlights that the data is incomplete for the year 2005.

#### Dealsize Distribution
"""

plt.figure(figsize=(4,3))
df['DEALSIZE'].value_counts(normalize = True).plot(kind = 'bar')
plt.title('DealSize distribution')
plt.xlabel('Deal Size')
plt.ylabel('% Proportion')
plt.show()

"""Majority of the deal size taken from the customers is found out to be 'Medium'.

#### Productline distribution
"""

plt.figure(figsize=(5,3))
df['PRODUCTLINE'].value_counts(normalize = True).plot(kind = 'bar')
plt.title('Productline distribution')
plt.xlabel('Productline')
plt.ylabel('% Proportion')
plt.show()

"""The 'Classic Cars' productline has the most number of purchase recorded in the considered time period. This plot talks about demand and clearly, the 'Classic Cars' has the most demand among the other services that the company provides."""

D=df.groupby(['YEAR_ID','QTR_ID'])[['SALES']].sum()
D

"""The above output depicts the year and further quarter-wise sales."""

df.groupby(['YEAR_ID'])[['SALES']].mean()

plt.figure(figsize=(5,3))
df.groupby(['YEAR_ID'])[['SALES']].mean().plot()
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.title('Annual Revenue')
plt.xticks(np.arange(2003,2006,1))
plt.show()

"""The above graph is misleading. The reason being the observations for the year 2005 is not complete, resulting in less number of minthly data. So, this affects the average wrongly and hence the misleading output."""

D.reset_index(inplace=True)
D.head()

plt.figure(figsize=(5,3))
sns.factorplot(y='SALES', x='QTR_ID',data=D,kind="bar" ,hue='YEAR_ID')

"""An interesting trend in the sales observed is that the sales is at its highest during the 4th quarter when compared to that of the first three quarters. The reasons could range from discount at the year end practice to the festive seasons that usually affects the sales."""

plt.figure(figsize=(5,4), facecolor= 'w', dpi=100)
sns.lineplot(x='ORDERDATE', y='SALES', data=df)

"""From the lineplot, it can be said that the 'Sales' fluctuates throughout the years considered.

#### Annual Revenue
"""

#Annual Revenue
plt.figure(figsize=(5,4))
df.groupby(['YEAR_ID'])['SALES'].sum().plot()
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.title('Annual Revenue')
plt.xticks(np.arange(2003,2006,1))
plt.show()

"""The above graph is not reliable and possibly could be misleading as as we dont have the complete data for 2005.
Analyzing Monthy Revenue could be a better option.

#### Monthly Revenue
"""

#Monthly Revenue
plt.figure(figsize=(5,4))

monthly_revenue_= df.groupby(['YEAR_ID','MONTH_ID'])['SALES'].sum().reset_index()
monthly_revenue_
sns.lineplot(x="MONTH_ID", y="SALES",hue="YEAR_ID", data=monthly_revenue_)
plt.xlabel('Month')
plt.ylabel('Sales')
plt.title('Monthly Revenue')
plt.show()

"""The above procured plot depicts that the revenue is growing especially in the months of October and November. The reason could possibly be due to seasonality(result of festivals). Also, 2005 is performing better than the other years in terms of revenue reaching the maximum sales in all the months that is from Jan to May. The reason behind this increase of sales in 2005 can be further examined in order to maintain high sales in future of the company .

#### Monthly Revenue Growth Rate
"""

monthly_revenue_['MONTHLY_GROWTH'] = monthly_revenue_['SALES'].pct_change()

monthly_revenue_.head()

#Monthly Sales Growth Rate
plt.figure(figsize=(5,4))
sns.lineplot(x="MONTH_ID", y="MONTHLY_GROWTH",hue="YEAR_ID", data=monthly_revenue_)
plt.xlabel('Month')
plt.ylabel('Sales')
plt.title('Monthly Sales Growth Rate')
plt.show()

"""A high growth rate from Apr 2005 to May 2005 is seen from the above plot.

#### Top 10 countries by Sales
"""

plt.figure(figsize=(5,4))
top_cities = df.groupby(['COUNTRY'])['SALES'].sum().sort_values(ascending=False)
top_cities.plot(kind = 'bar')
plt.title('Top 10 countries by Sales')
plt.xlabel('Country')
plt.ylabel('Total Sales')
plt.show()

"""USA has the highest sales recorded .

#### Average Sales per Order
"""

#Average Sales per Order
average_revenue = df.groupby(['YEAR_ID','MONTH_ID'])['SALES'].mean().reset_index()
plt.figure(figsize=(5,4))
sns.lineplot(x="MONTH_ID", y="SALES",hue="YEAR_ID", data=average_revenue)
plt.xlabel('Month')
plt.ylabel('Average Sales')
plt.title('Average Sales per Order')
plt.show()

"""#### Monthly Active Customers"""

plt.figure(figsize=(5,4))
df['YEAR_MONTH'] = df['YEAR_ID'].map(str)+df['MONTH_ID'].map(str).map(lambda x: x.rjust(2,'0'))
monthly_active = df.groupby(['YEAR_MONTH'])['CUSTOMERNAME'].nunique().reset_index()
monthly_active.plot(kind='bar',x='YEAR_MONTH',y='CUSTOMERNAME')
#plt.figure(figsize=(10,8))
plt.title('Monthly Active Customers')
plt.xlabel('Month/Year')
plt.ylabel('Number of Unique Customers')
plt.xticks(rotation=90)
#plt.figure(figsize=(10,8))
plt.show()

"""As expected, customers are highly active during the months of November and October. The number of active customers increased from 2003 to 2004 which indicates that the company is successful in retention/acquisition of ol/new customers."""

#Correlation matrix
plt.figure(figsize=(5,4))
corr_matrix = df.iloc[:, :10].corr()
sns.heatmap(corr_matrix, annot=True);

"""* From the above output, it can be seen that MSRP is positively correlated to PRICEEACH and SALES.
* PRODUCTCODE is negatively correlated with MSRP, PRICEEACH and SALES.
* There exists positive correlation btw SALES, PRICEEACH, QUANTITYORDERED

### Findings:-

* Most of the orders' quantity lie between 20 and 40 units. Therefore, majority of them are bulk orders.
* Most of the orders placed are of quantities costing between 90 and100and100  per item. Also, the maximum price per item in the records is 100$.
* Majority of the customer sales are below the mean sales of the time period considered.
* A huge majority of the orders/products have been successfully shipped. While, there was Dispute, In process and Onhold status in 2005. This highlights the performance of the company shipping in 2005.(in need of improvement)
* Majority of the deal size taken from the customers is found out to be 'Medium'.
* The 'Classic Cars' productline has the most number of purchase recorded in the considered time period. This plot talks about demand and clearly, the 'Classic Cars' has the most demand among the other services that the company provides.
* An interesting trend in the sales observed is that the sales is at its highest during the 4th quarter when compared to that of the first three quarters. The reasons could range from discount at the year end practice to the festive seasons that usually affects the sales.
* The revenue is growing especially in the months of October and November. The reason could possibly be due to seasonality(result of festivals). Also, 2005 is performing better than the other years in terms of revenue reaching the maximum sales in all the months that is from Jan to May. The reason behind this increase of sales in 2005 can be further examined in order to maintain high sales in future of the company .
* A high growth rate from Apr 2005 to May 2005 is seen .
* USA has the highest sales recorded .
* Customers are highly active during the months of November and October. The number of active customers increased from 2003 to 2004 which indicates that the company is successful in retention/acquisition of ol/new customers.
* From the above output, it can be seen that MSRP is positively correlated to PRICEEACH and SALES.
PRODUCTCODE is negatively correlated with MSRP, PRICEEACH and SALES.
There exists positive correlation btw SALES, PRICEEACH, QUANTITYORDERED
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
#datetime
import datetime as dt
#StandardSccaler
from sklearn.preprocessing import StandardScaler
#KMeans
from sklearn.cluster import KMeans
from mpl_toolkits.mplot3d import Axes3D
from yellowbrick.cluster import KElbowVisualizer

"""##### Segmentation using KMeans Clustering"""

df['ORDERDATE'] = [d.date() for d in df['ORDERDATE']]
df.head()

"""##### *Calculate Recency, Frequency and Monetary value for each customer*:

Assuming that we are analyzing the next day of latest order date in the data set, creating a variable 'snapshot date' which is the latest date in data set.

Recency : Recency is the number of days between the customer's latest order date and the snapshot date
Frequency: Number of purchases made by the customer
MonetaryValue: Revenue generated by the customer
"""

# Calculate Recency, Frequency and Monetary value for each customer
snapshot_date = df['ORDERDATE'].max() + dt.timedelta(days=1) #latest date in the data set
df_RFM = df.groupby(['CUSTOMERNAME']).agg({
    'ORDERDATE': lambda x: (snapshot_date - x.max()).days,
    'ORDERNUMBER': 'count',
    'SALES':'sum'})

#Renaming the columns
df_RFM.rename(columns={'ORDERDATE': 'Recency','ORDERNUMBER': 'Frequency','SALES': 'MonetaryValue'}, inplace=True)

df_RFM.head()

"""Data Preprocessing for KMeans

---


K Means Assumptions:
- All variables have symmetrical (Normal) Distribution
- All Variables have same average value(approx)
- All Variables have same variance(approx)
"""

#Check the distribution of the variables
data=df_RFM.copy()
plt.figure(figsize=(5,4))
plt.subplot(1,3,1)
data['Recency'].plot(kind='hist')
plt.title('Recency')
plt.subplot(1,3,2)
data['Frequency'].plot(kind='hist')
plt.title('Frequency')
plt.subplot(1,3,3)
data['MonetaryValue'].plot(kind='hist')
plt.xticks(rotation = 90)
plt.title('MonetaryValue')
plt.tight_layout()
plt.show()

#Removing the skewness by performing log transformation on the variables
data_log = np.log(data)
data_log.head()

#Distribution of Recency, Frequency and MonetaryValue after Log Transformation
plt.figure(figsize=(5,4))
sns.distplot(data_log['Recency'],label='Recency')
sns.distplot(data_log['Frequency'],label='Frequency')
sns.distplot(data_log['MonetaryValue'],label='MonetaryValue')
plt.title('Distribution of Recency, Frequency and MonetaryValue after Log Transformation')
plt.legend()
plt.show()

#Standardizing the variables using StandardScaler() for equal variance and mean
# Initialize a scaler
scaler = StandardScaler()
# Fit the scaler
scaler.fit(data_log)
# Scale and center the data
data_normalized = scaler.transform(data_log)
# Create a pandas DataFrame
data_normalized = pd.DataFrame(data_normalized, index=data_log.index, columns=data_log.columns)
# Print summary statistics
data_normalized.describe().round(2)

"""*Choosing number of Clusters using Elbow Method*"""

# Fit KMeans and calculate SSE for each k
sse={}
for k in range(1, 21):
    kmeans = KMeans(n_clusters=k, random_state=1)
    kmeans.fit(data_normalized)
    sse[k] = kmeans.inertia_     
plt.figure(figsize=(5,5))
# Add the plot title "The Elbow Method"
plt.title('The Elbow Method')
# Add X-axis label "k"
plt.xlabel('k')
# Add Y-axis label "SSE"
plt.ylabel('SSE')
# Plot SSE values for each key in the dictionary
sns.pointplot(x=list(sse.keys()), y=list(sse.values()))
plt.show()

from sklearn.metrics import silhouette_score
for n_cluster in range(2, 11):
    kmeans = KMeans(n_clusters=n_cluster).fit(data_normalized)
    label = kmeans.labels_
    sil_coeff = silhouette_score(data_normalized, label, metric='euclidean')
    print("For n_clusters={}, The Silhouette Coefficient is {}".format(n_cluster, sil_coeff))

fig=plt.figure(figsize=(5,4))

# Instantiate the clustering model and visualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(2,6),metric='silhouette',timings= True,locate_elbow=True)

visualizer.fit(data_normalized)    # Fit the data to the visualizer
visualizer.poof()

"""*Running KMeans with 4 clusters*"""

# Initialize KMeans
kmeans = KMeans(n_clusters=4, random_state=1) 
# Fit k-means clustering on the normalized data set
kmeans.fit(data_normalized)
# Extract cluster labels
cluster_labels = kmeans.labels_

# Assigning Cluster Labels to Raw Data
# Create a DataFrame by adding a new cluster label column
data_rfm = data.assign(Cluster=cluster_labels)
data_rfm.head()

# Group the data by cluster
grouped = data_rfm.groupby(['Cluster'])
# Calculate average RFM values and segment sizes per cluster value
grouped.agg({'Recency': 'mean','Frequency': 'mean','MonetaryValue': ['mean', 'count']}).round(1)

# Calculate average RFM values for each cluster
cluster_avg = data_rfm.groupby(['Cluster']).mean() 
print(cluster_avg)

# %matplotlib notebook    #to make plot interactive
data_normalized=pd.DataFrame(data_normalized)
data_normalized['Cluster']=data_rfm['Cluster'].values

data_normalized.head()

fig=plt.figure()
ax=Axes3D(fig)
for x in range(0,4):
    ax.scatter(data_normalized.loc[ data_normalized['Cluster']==x]['Recency'],
               data_normalized.loc[ data_normalized['Cluster']==x]['Frequency'],
               data_normalized.loc[data_normalized['Cluster']==x]['MonetaryValue'],
               label=f'Cluster {x}')
    ax.legend()
ax.set_xlabel('Recency')
ax.set_ylabel('Frequency')
ax.set_zlabel('Monetry value')
plt.show()

fig = plt.figure(figsize=(12,8))

plt.scatter(data_normalized.values[:,0], data_normalized.values[:,1], c=kmeans.labels_, cmap="Set1_r", s=25)
plt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='black', marker="x", s=250)
plt.title("Kmeans Clustering \n Finding Unknown Groups in the Population", fontsize=16)
plt.show()

"""K modes

---
The k-modes algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function. 



"""

application = pd.read_excel('/content/drive/My Drive/application_record.xlsx')
application.head()

credit = pd.read_excel('/content/drive/My Drive/credit_record.xlsx')
credit.head()

application.shape

#Finding missing values
application.isnull().sum()

del application['OCCUPATION_TYPE']

#Check for duplicate records
application['ID'].nunique() # the total rows are 438,557. This means it has duplicates

application = application.drop_duplicates('ID', keep='last') 
# we identified that there are some duplicates in this dataset
# we will be deleting those duplicates and will keep the last entry of the ID if its repeated.

#Count of children
fig=plt.figure(figsize=(5,4))
sns.countplot(x="CNT_CHILDREN",data=application,linewidth=2,edgecolor=sns.color_palette("dark", 1))
CNT_CHILDREN = application['CNT_CHILDREN'].value_counts()
for a,b in zip(range(len(CNT_CHILDREN)), CNT_CHILDREN):
    plt.text(a, b+50, '%.0f' % b, ha='center', va= 'bottom',fontsize=14)
plt.show()

#Count of family members
plt.figure(figsize=(5,4))
sns.countplot(x="CNT_FAM_MEMBERS", data=application, palette="Greens_d")
CNT_FAM_MEMBERS = application.CNT_FAM_MEMBERS.apply(int).value_counts().sort_index()
for a,b in zip(range(len(CNT_FAM_MEMBERS)), CNT_FAM_MEMBERS):
    plt.text(a, b+50, '%.0f' % b, ha='center', va= 'bottom',fontsize=12)

plt.show()

#Distribution of Total income
fig = plt.figure(figsize=(5,4))
sns.distplot(application['AMT_INCOME_TOTAL'])
plt.title('Income Distribution')
plt.xlabel('Income')
plt.ylabel('Frequency')
plt.show()

fig = plt.figure(figsize=(5,4))
age = application.DAYS_BIRTH.apply(lambda x: int(-x / 365.25))
age_plot = pd.Series(age, name="age")
sns.distplot(age_plot)
plt.show()

#The number of the days from employed
fig = plt.figure(figsize=(5,4))
employed_year = application[application.DAYS_EMPLOYED<0].DAYS_EMPLOYED.apply(lambda x: int(-x // 365.25))
employed_plot = pd.Series(employed_year, name="employed_year")
sns.distplot(employed_plot)
plt.show()

#Gender proportion in applicants
plt.figure(figsize=(5,2))
gender_val = application.CODE_GENDER.value_counts(normalize = True)
gender_val
gender_val.plot.pie()
plt.show()

"""Around 67.14% of the applicants are female"""

#Applicants with cars
fig = plt.figure(figsize=(3,3))
x = application['FLAG_OWN_CAR'].value_counts()
sns.barplot(x.index,x)

#Applicant realty
fig = plt.figure(figsize=(3,3))
x = application['FLAG_OWN_REALTY'].value_counts()
sns.barplot(x.index,x)

#Type of income
fig = plt.figure(figsize=(9,3))
x = application['NAME_INCOME_TYPE'].value_counts()
sns.barplot(x.index,x)

#Level of education
fig = plt.figure(figsize=(12,3))
x = application['NAME_EDUCATION_TYPE'].value_counts()
sns.barplot(x.index,x)

#Education level and income relation
fig = plt.figure(figsize=(12,2))
application.groupby(["NAME_EDUCATION_TYPE"]).AMT_INCOME_TOTAL.mean().sort_values(ascending=False).plot.barh()
plt.show()

#Type of family status
fig = plt.figure(figsize=(12,2))
x = application['NAME_FAMILY_STATUS'].value_counts()
sns.barplot(x.index,x)

#Applicants' housing type
fig = plt.figure(figsize=(12,3))
x = application['NAME_HOUSING_TYPE'].value_counts()
sns.barplot(x.index,x)

#Outlier detection
d=application[['AMT_INCOME_TOTAL','DAYS_BIRTH','DAYS_EMPLOYED']]
d.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))
plt.show()

#Outlier removal(not used,just checking)
print('original shape of dataset :',d.shape)
#criteria
Q1 = d.quantile(0.25)
Q3 = d.quantile(0.75)
IQR = Q3-Q1
max_ = Q3+1.5*IQR
min_ = Q1-1.5*IQR
#filter the outlier s
condition = (d <= max_) & (d >= min_)
condition = condition.all(axis=1)
df = d[condition]
print('filtered dataset shape : ',df.shape)
df.plot(kind='box', subplots=True, sharex=False, sharey=False, figsize=(10,10), layout=(3,4))
plt.show()

fig=plt.figure(figsize=(5,4))
qqplot(application[['AMT_INCOME_TOTAL']], line='s')
plt.show()

application['DAYS_EMPLOYED']=application[['DAYS_EMPLOYED']]*-1
application['DAYS_BIRTH']=application[['DAYS_BIRTH']]*-1
application.head()

"""Since the features are not normally distibuted, we cannot standardize the dataset, instead we normalize. The dataset will be normalised after after the 2 sub-datasets-'application' and 'credit'.

##### Credit record
This is a csv file with credit record for a part of ID in application record. We can treat it a file to generate labels for modeling. For the applicants who have a record more than 59 past due, they should be rejected

* ID: Unique Id of the row in application record.
* MONTHS_BALANCE: The number of months from record time.
* STATUS: Credit status for this month.


  X: No loan for the month
  C: paid off that month 
  0: 1-29 days past due 
  1: 30-59 days past due 
  2: 60-89 days overdue
  3: 90-119 days overdue 
  4: 120-149 days overdue 
  5: Overdue or bad debts, write-offs for more than 150 days
"""

credit.shape

#Checking for inconsistent data types
credit.info()

#Check for duplicate records
credit['ID'].nunique() 
# this has around 45,000 unique rows as there are repeating entries for different monthly values and status.

#Finding missing values
credit.isnull().sum()

#records matching in two datasets
len(set(credit['ID']).intersection(set(application['ID'])))

fig = plt.figure(figsize=(7,4))
ax = fig.add_subplot()
ax.set_title('Correlation Plot', fontsize=18)
sns.heatmap(application[['CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'FLAG_WORK_PHONE', 
                         'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS']].corr(), ax=ax)

"""There's a strong correlation between DAYS_EMPLOYED and DAYS_BIRTH, CNT_FAM_MEMBERS and CNT_CHIDREN"""

#calculating months from today column to see how much old the month is 
credit['Months_from_today'] = credit['MONTHS_BALANCE']*-1
credit = credit.sort_values(['ID','Months_from_today'], ascending=True)
credit.head(3)

del credit['MONTHS_BALANCE']

application1=application.copy()

"""**Understanding the Response variable- 'Status':**

---






"""

credit['STATUS'].value_counts()

#Status will be the label/prediction result for our model
#Replacing the value C and X with 0 as it is the same type and 1,2,3,4,5 are classified as 1 because they are the same type
credit['STATUS'].replace({'C': 0, 'X' : 0}, inplace=True)
credit['STATUS'] = credit['STATUS'].astype('int')
credit['STATUS'] = credit['STATUS'].apply(lambda x:1 if x >= 2 else 0)

#Normalizing the Status counts
credit['STATUS'].value_counts(normalize=True)

"""It can be see that the data is oversampled for the labels since 
'0' are 99% '1' are only 1% in the whole dataset.This oversampling issue needs to be addressed in order to make sense of the further analysis.This problem will be dealt with after combining both datasets.
"""

#Grouping the data in 'credit' dataset by 'ID' so that we can join it with 'application' dataset
credit_grp = credit.groupby('ID').agg(max).reset_index()
credit_grp.head()

# Combining the datasets
df = application1.join(credit_grp.set_index('ID'), on='ID', how='inner')
df.head()

## **Differentiating the features as Binary, Numerical and Categorical**
#filtering the columns that have non numeric values
obj = pd.DataFrame(application.dtypes =='object').reset_index()
object_type = obj[obj[0] == True]['index']
object_type=pd.DataFrame(object_type)

binary=[]
cat=[]
num=[]

#Deciding if features are Binary or Categorical
for i in range(len(object_type)):
  col=object_type.iloc[i,0]
  if application[col].value_counts().count()<3:
    binary.append(col)
  else:
    cat.append(col)

##filtering the columns that have numeric values
num_type = pd.DataFrame(application.dtypes != 'object').reset_index()
num_type = num_type[num_type[0] ==True]['index']
##Deciding if features are Binary or Categorical
num_type=pd.DataFrame(num_type)
for i in range(len(num_type)):
  col=num_type.iloc[i,0]
  if application[col].value_counts().count()<3:
    binary.append(col)
  else:
    num.append(col)

for i in cat:
  print(i)

# Combining the datasets
df1 = application1.join(credit_grp.set_index('ID'), on='ID', how='inner')
df1.head()

cate=df1[cat].copy()
cate.head()

#get_dummies() function is named this way because it creates dummy/indicator variables (aka 1 or 0).
cate1=pd.get_dummies(cate, columns=cate.columns,prefix=["Income_type","Edu_type","Fam_stat","House_type"])

for i in binary:
  print(i)

binar=df1[binary].copy()

#LabelEncoding-transforming all the non numeric data columns into datacolumns of 0 and 1s
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for x in binary:
        binar[x] = le.fit_transform(binar[x])

cate1.reset_index(drop=True, inplace=True)
binar.reset_index(drop=True, inplace=True)
data_m=pd.concat([cate1,binar],axis=1)
data_m.head()

data_m.shape

!pip install kmodes

plt.figure(figsize=(5,4))
cost = []
K = range(1,7)
for num_clusters in list(K):
    kmode = KModes(n_clusters=num_clusters, init = "Cao", n_init = 1, verbose=1)
    kmode.fit_predict(data_m)
    cost.append(kmode.cost_)
    
plt.plot(K, cost, 'bx-')
plt.xlabel('k clusters')
plt.ylabel('Cost')
plt.title('Elbow Method For Optimal k')
plt.show()

from sklearn.metrics import silhouette_score
for n_cluster in range(2, 6):
    kmode = KModes(n_clusters=n_cluster).fit(data_m)
    label = kmode.labels_
    sil_coeff = silhouette_score(data_m, label, metric='jaccard')
    print("For n_clusters={}, The Silhouette Coefficient is {}".format(n_cluster, sil_coeff))

from kmodes.kmodes import KModes
km_cao = KModes(n_clusters=3, init = "Cao", n_init = 1, verbose=1)
fitClusters_cao = km_cao.fit_predict(data_m)

# Predicted Clusters
fitClusters_cao

clusterCentroidsDf = pd.DataFrame(km_cao.cluster_centroids_)
clusterCentroidsDf.columns = data_m.columns

# Mode of the clusters
clusterCentroidsDf

clustersDf = pd.DataFrame(fitClusters_cao)
clustersDf.columns = ['cluster_predicted']
data_m.reset_index(drop=True, inplace=True)
clustersDf.reset_index(drop=True, inplace=True)
combinedDf = pd.concat([data_m, clustersDf], axis = 1).reset_index()
combinedDf = combinedDf.drop('index', axis = 1)

combinedDf.head()

cluster_0 = combinedDf[combinedDf['cluster_predicted'] == 0]
cluster_1 = combinedDf[combinedDf['cluster_predicted'] == 1]
cluster_2= combinedDf[combinedDf['cluster_predicted'] == 2]

print(cluster_0.head().shape)
print(cluster_1.head().shape)
print(cluster_2.head().shape)

for i in combinedDf:
  sns.countplot(x=combinedDf[i],order=combinedDf[i].value_counts().index,hue=combinedDf['cluster_predicted'])
  plt.show()

"""K Modes

---


Advantages

* It is widely used algorithm for grouping the categorical data because it is easy to implement and efficiently handles large amount of data.



Disadvantages

* The efficiency of the method depends on the definition of distance.

K Means

---


Advantages


* The k-means algorithm is well known for its efficiency in clustering large data sets. 
* Relatively simple to implement.
* Easily adapts to new examples.
* K means Generalization - It generalizes to clusters of different shapes and sizes, such as elliptical clusters.
* If variables are huge, then K-Means most of the times computationally faster than hierarchical clustering and also k mode clustering.


Disadvantages


* Working only on numeric values prohibits it from being used to cluster real world data containing categorical values.

* Clustering outliers- Centroids can be dragged by outliers, or outliers might get their own cluster instead of being ignored.
"""

